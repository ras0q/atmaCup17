{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = path.join(Path().resolve(), \"data\")\n",
    "dataset_dir = path.join(data_dir, \"dataset\")\n",
    "\n",
    "csv_train = pd.read_csv(path.join(dataset_dir, \"train.csv\"))\n",
    "csv_test = pd.read_csv(path.join(dataset_dir, \"test.csv\"))\n",
    "csv_clothing_master = pd.read_csv(path.join(dataset_dir, \"clothing_master.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Division Name\", \"Department Name\", \"Class Name\"]:\n",
    "    df_encoded = pd.get_dummies(\n",
    "        csv_clothing_master[col].astype(str), prefix=col\n",
    "    ).astype(int)\n",
    "    csv_clothing_master = csv_clothing_master.drop(col, axis=1)\n",
    "    csv_clothing_master = pd.concat([csv_clothing_master, df_encoded], axis=1)\n",
    "\n",
    "\n",
    "def preprocess(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    x = x.copy()\n",
    "    x = x.merge(csv_clothing_master, on=\"Clothing ID\")\n",
    "    x = x.drop(\n",
    "        [\n",
    "            \"Title\",\n",
    "            \"Review Text\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "train = preprocess(csv_train)\n",
    "test = preprocess(csv_test)\n",
    "\n",
    "train_x, train_y = (\n",
    "    train.drop([\"Recommended IND\", \"Rating\"], axis=1),  # TODO: utilize \"Rating\"\n",
    "    train[\"Recommended IND\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import lightning as L\n",
    "\n",
    "L.seed_everything(42, workers=True)\n",
    "\n",
    "train_val = data.TensorDataset(\n",
    "    torch.tensor(train_x.values, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "train_size = int(len(train_val) * 0.9)\n",
    "val_size = len(train_val) - train_size\n",
    "dataset_train, dataset_val = data.random_split(\n",
    "    train_val, lengths=[train_size, val_size]\n",
    ")\n",
    "dataset_test = data.TensorDataset(torch.tensor(test.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "from torch.optim import optimizer\n",
    "\n",
    "\n",
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.functional.mse_loss\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self) -> optimizer.Optimizer:\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, *_ = batch\n",
    "        output = self(inputs)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mras0q\u001b[0m (\u001b[33mras0q-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/kira/ghq/github.com/ras0q/atmaCup17/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240831_185400-nqi0ewhg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ras0q-team/atmaCup17/runs/nqi0ewhg' target=\"_blank\">atmaCup17</a></strong> to <a href='https://wandb.ai/ras0q-team/atmaCup17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ras0q-team/atmaCup17' target=\"_blank\">https://wandb.ai/ras0q-team/atmaCup17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ras0q-team/atmaCup17/runs/nqi0ewhg' target=\"_blank\">https://wandb.ai/ras0q-team/atmaCup17/runs/nqi0ewhg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 1.9 K  | train\n",
      "---------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326627/2363724382.py:35: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/282 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326627/2363724382.py:21: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 282/282 [00:01<00:00, 197.19it/s, v_num=ewhg, train_loss=0.209] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326627/2363724382.py:21: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 282/282 [00:01<00:00, 174.21it/s, v_num=ewhg, train_loss=0.209, val_loss_step=0.177, val_loss_epoch=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_326627/2363724382.py:35: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 282/282 [00:01<00:00, 148.96it/s, v_num=ewhg, train_loss=0.189, val_loss_step=0.154, val_loss_epoch=0.167] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 282/282 [00:02<00:00, 136.51it/s, v_num=ewhg, train_loss=0.189, val_loss_step=0.154, val_loss_epoch=0.167]\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "logger = WandbLogger(name=\"atmaCup17\", project=\"atmaCup17\", log_model=\"all\")\n",
    "\n",
    "model = LightningModel(\n",
    "    nn.Sequential(nn.Linear(len(train_x.columns), 64), nn.ReLU(), nn.Linear(64, 1))\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=20,\n",
    "    log_every_n_steps=10,\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=data.DataLoader(\n",
    "        dataset_train, batch_size=batch_size, num_workers=num_workers, pin_memory=True\n",
    "    ),\n",
    "    val_dataloaders=data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, num_workers=num_workers, pin_memory=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 349/349 [00:00<00:00, 402.85it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(\n",
    "    model,\n",
    "    data.DataLoader(\n",
    "        dataset_test, batch_size=batch_size, num_workers=num_workers, pin_memory=True\n",
    "    ),\n",
    ")\n",
    "assert outputs is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "with open(path.join(data_dir, f\"submit_{int(time.time())}.csv\"), \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"target\"])\n",
    "    writer.writerows([[np.mean(x.tolist())] for x in outputs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
