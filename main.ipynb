{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "now = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = Path().cwd() / \"data\"\n",
    "dataset_dir = data_dir / \"dataset\"\n",
    "\n",
    "csv_train = pd.read_csv(dataset_dir / \"train.csv\")\n",
    "csv_test = pd.read_csv(dataset_dir / \"test.csv\")\n",
    "csv_clothing_master = pd.read_csv(dataset_dir / \"clothing_master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "for col in [\"Division Name\", \"Department Name\", \"Class Name\"]:\n",
    "    df_encoded = pd.get_dummies(\n",
    "        csv_clothing_master[col].astype(str),\n",
    "        prefix=col,\n",
    "    ).astype(int)\n",
    "    csv_clothing_master = csv_clothing_master.drop(col, axis=1)\n",
    "    csv_clothing_master = pd.concat([csv_clothing_master, df_encoded], axis=1)\n",
    "\n",
    "\n",
    "def preprocess(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    x = x.copy()\n",
    "    x = x.merge(csv_clothing_master, on=\"Clothing ID\")\n",
    "    x = x.drop(\n",
    "        [\n",
    "            \"Clothing ID\",\n",
    "            \"Title\",\n",
    "            \"Review Text\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    cols = [\n",
    "        \"Age\",\n",
    "        \"Positive Feedback Count\",\n",
    "    ]\n",
    "    x[cols] = scaler.fit_transform(x[cols])\n",
    "    return x\n",
    "\n",
    "\n",
    "train = preprocess(csv_train)\n",
    "test = preprocess(csv_test)\n",
    "\n",
    "train_x, train_y = (\n",
    "    train.drop([\"Recommended IND\", \"Rating\"], axis=1),\n",
    "    train[\"Recommended IND\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import lightning as pl\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "train_val = data.TensorDataset(\n",
    "    torch.tensor(train_x.values, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "train_size = int(len(train_val) * 0.9)\n",
    "val_size = len(train_val) - train_size\n",
    "dataset_train, dataset_val = data.random_split(\n",
    "    train_val,\n",
    "    lengths=[train_size, val_size],\n",
    ")\n",
    "dataset_test = data.TensorDataset(torch.tensor(test.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.optim import optimizer\n",
    "\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.functionapl.mse_loss\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self) -> optimizer.Optimizer:\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, *_ = batch\n",
    "        return self(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/kira/ghq/github.com/ras0q/atmaCup17/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240831_192642-mei2u3w4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ras0q-team/atmaCup17/runs/mei2u3w4' target=\"_blank\">atmaCup17_1725099991</a></strong> to <a href='https://wandb.ai/ras0q-team/atmaCup17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ras0q-team/atmaCup17' target=\"_blank\">https://wandb.ai/ras0q-team/atmaCup17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ras0q-team/atmaCup17/runs/mei2u3w4' target=\"_blank\">https://wandb.ai/ras0q-team/atmaCup17/runs/mei2u3w4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 1.9 K  | train\n",
      "---------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336103/2363724382.py:35: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 2/282 [00:00<00:15, 18.32it/s, v_num=u3w4, train_loss=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336103/2363724382.py:21: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 282/282 [00:01<00:00, 187.41it/s, v_num=u3w4, train_loss=0.191] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336103/2363724382.py:21: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 282/282 [00:01<00:00, 161.00it/s, v_num=u3w4, train_loss=0.191, val_loss_step=0.114, val_loss_epoch=0.146]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336103/2363724382.py:35: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 282/282 [00:01<00:00, 143.99it/s, v_num=u3w4, train_loss=0.192, val_loss_step=0.112, val_loss_epoch=0.145] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 282/282 [00:02<00:00, 131.23it/s, v_num=u3w4, train_loss=0.192, val_loss_step=0.112, val_loss_epoch=0.145]\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "logger = WandbLogger(name=f\"atmaCup17_{now}\", project=\"atmaCup17\", log_model=\"all\")\n",
    "\n",
    "model = LightningModel(\n",
    "    nn.Sequential(nn.Linear(len(train_x.columns), 64), nn.ReLU(), nn.Linear(64, 1)),\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=20,\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    ),\n",
    "    val_dataloaders=data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 349/349 [00:00<00:00, 452.30it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(\n",
    "    model,\n",
    "    data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    ),\n",
    ")\n",
    "if outputs is None:\n",
    "    msg = \"none output\"\n",
    "    raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with (data_dir / f\"submit_{now}.csv\").open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"target\"])\n",
    "    writer.writerows([[np.mean(x.tolist())] for x in outputs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
