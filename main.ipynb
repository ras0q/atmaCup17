{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = path.join(Path().resolve(), \"data\")\n",
    "dataset_dir = path.join(data_dir, \"dataset\")\n",
    "\n",
    "csv_train = pd.read_csv(path.join(dataset_dir, \"train.csv\"))\n",
    "csv_test = pd.read_csv(path.join(dataset_dir, \"test.csv\"))\n",
    "csv_clothing_master = pd.read_csv(path.join(dataset_dir, \"clothing_master.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import lightning as L\n",
    "\n",
    "L.seed_everything(42, workers=True)\n",
    "\n",
    "\n",
    "def preprocess(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    x = x.copy()\n",
    "    x = x.merge(csv_clothing_master, on=\"Clothing ID\")\n",
    "    x = x.drop(\n",
    "        columns=[\n",
    "            \"Title\",\n",
    "            \"Review Text\",\n",
    "            \"Division Name\",\n",
    "            \"Department Name\",\n",
    "            \"Class Name\",\n",
    "        ]\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "train = preprocess(csv_train)\n",
    "test = preprocess(csv_test)\n",
    "\n",
    "train_x, train_y = (\n",
    "    train.drop(columns=[\"Recommended IND\", \"Rating\"]),  # TODO: utilize \"Rating\"\n",
    "    train[\"Recommended IND\"],\n",
    ")\n",
    "\n",
    "train_val = data.TensorDataset(\n",
    "    torch.tensor(train_x.values, dtype=torch.float32),\n",
    "    torch.tensor(train_y, dtype=torch.float32),\n",
    ")\n",
    "\n",
    "train_size = int(len(train_val) * 0.9)\n",
    "val_size = len(train_val) - train_size\n",
    "dataset_train, dataset_val = data.random_split(\n",
    "    train_val, lengths=[train_size, val_size]\n",
    ")\n",
    "dataset_test = data.TensorDataset(torch.tensor(test.values, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "from torch.optim import optimizer\n",
    "\n",
    "\n",
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.functional.mse_loss\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self) -> optimizer.Optimizer:\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "        inputs, *_ = batch\n",
    "        output = self(inputs)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kira/ghq/github.com/ras0q/atmaCup17/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kira/ghq/github.com/ras0q/atmaCup17/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 321    | train\n",
      "---------------------------------------------\n",
      "321       Trainable params\n",
      "0         Non-trainable params\n",
      "321       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292566/3452656499.py:28: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n",
      "/home/kira/ghq/github.com/ras0q/atmaCup17/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 6/9000 [00:00<02:18, 64.76it/s, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292566/3452656499.py:21: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = self.loss_fn(output, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9000/9000 [00:42<00:00, 210.67it/s, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9000/9000 [00:42<00:00, 210.64it/s, v_num=10]\n"
     ]
    }
   ],
   "source": [
    "model = LightningModel(\n",
    "    nn.Sequential(nn.Linear(len(train_x.columns), 64), nn.ReLU(), nn.Linear(64, 1))\n",
    ")\n",
    "trainer = L.Trainer(accelerator=\"gpu\", max_epochs=2, log_every_n_steps=10, devices=1)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataset_train, val_dataloaders=dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 11155/11155 [00:11<00:00, 950.68it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(model, dataloaders=dataset_test)\n",
    "assert outputs is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "with open(path.join(data_dir, f\"submit_{int(time.time())}.csv\"), \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"target\"])\n",
    "    writer.writerows([x.tolist() for x in outputs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
